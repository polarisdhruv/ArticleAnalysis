{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed for https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Request failed for https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to extract article text using BeautifulSoup\n",
    "def extract_article_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract the title\n",
    "        title_tag = soup.find('h1')\n",
    "        title = title_tag.get_text() if title_tag else 'No Title'\n",
    "        \n",
    "        # Extract the article text\n",
    "        article_text = ''\n",
    "        for p in soup.find_all('p'):\n",
    "            article_text += p.get_text() + '\\n'\n",
    "        \n",
    "        return title, article_text\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed for {url}: {e}\")\n",
    "        return 'No Title', ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing {url}: {e}\")\n",
    "        return 'No Title', ''\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    # Read the input Excel file\n",
    "    input_df = pd.read_excel('input.xlsx')\n",
    "\n",
    "    # Create a directory to save the articles\n",
    "    if not os.path.exists('articles'):\n",
    "        os.makedirs('articles')\n",
    "\n",
    "    # Extract and save articles\n",
    "    for index, row in input_df.iterrows():\n",
    "        url_id = row['URL_ID']\n",
    "        url = row['URL']\n",
    "        title, article_text = extract_article_text(url)\n",
    "        \n",
    "        # Save the extracted text to a file\n",
    "        with open(f'articles/{url_id}.txt', 'w', encoding='utf-8') as file:\n",
    "            file.write(title + '\\n\\n' + article_text)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load positive and negative words\n",
    "with open('MasterDictionary/positive-words.txt', 'r') as f:\n",
    "    positive_words = set(f.read().split())\n",
    "\n",
    "with open('MasterDictionary/negative-words.txt', 'r') as f:\n",
    "    negative_words = set(f.read().split())\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "additional_stopwords_files = [\n",
    "    'StopWords/StopWords_Auditor.txt', 'StopWords/StopWords_DatesandNumbers.txt', 'StopWords/StopWords_Generic.txt',\n",
    "    'StopWords/StopWords_Names.txt', 'StopWords/StopWords_GenericLong.txt', 'StopWords/StopWords_Currencies.txt', 'StopWords/StopWords_Geographic.txt'\n",
    "]\n",
    "\n",
    "for file_name in additional_stopwords_files:\n",
    "    with open(file_name, 'r') as f:\n",
    "        stop_words.update(f.read().split())\n",
    "\n",
    "def compute_sentiment_scores(text):\n",
    "    words = word_tokenize(text)\n",
    "    positive_score = sum(1 for word in words if word in positive_words)\n",
    "    negative_score = sum(1 for word in words if word in negative_words)\n",
    "    \n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (len(words) + 0.000001)\n",
    "    \n",
    "    return positive_score, negative_score, polarity_score, subjectivity_score\n",
    "\n",
    "def compute_readability_metrics(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    complex_word_count = sum(1 for word in words if len([char for char in word if char in 'aeiou']) > 2)\n",
    "    word_count = len(words)\n",
    "    sentence_count = len(sentences)\n",
    "    \n",
    "    avg_sentence_length = word_count / sentence_count if sentence_count else 0\n",
    "    percentage_complex_words = complex_word_count / word_count if word_count else 0\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "    \n",
    "    avg_words_per_sentence = word_count / sentence_count if sentence_count else 0\n",
    "    syllable_count_per_word = sum(len([char for char in word if char in 'aeiou']) for word in words) / word_count if word_count else 0\n",
    "    \n",
    "    return avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count, word_count, syllable_count_per_word\n",
    "\n",
    "def compute_personal_pronouns(text):\n",
    "    personal_pronouns = re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.IGNORECASE)\n",
    "    return len(personal_pronouns)\n",
    "\n",
    "def compute_avg_word_length(text):\n",
    "    words = word_tokenize(text)\n",
    "    total_characters = sum(len(word) for word in words)\n",
    "    avg_word_length = total_characters / len(words) if words else 0\n",
    "    return avg_word_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def analyze_article(text):\n",
    "#     positive_score, negative_score, polarity_score, subjectivity_score = compute_sentiment_scores(text)\n",
    "#     avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count, word_count, syllable_count_per_word = compute_readability_metrics(text)\n",
    "#     personal_pronouns_count = compute_personal_pronouns(text)\n",
    "#     avg_word_length = compute_avg_word_length(text)\n",
    "    \n",
    "#     return [\n",
    "#         positive_score, negative_score, polarity_score, subjectivity_score,\n",
    "#         avg_sentence_length, percentage_complex_words, fog_index,\n",
    "#         avg_words_per_sentence, complex_word_count, word_count,\n",
    "#         syllable_count_per_word, personal_pronouns_count, avg_word_length\n",
    "#     ]\n",
    "\n",
    "# def main():\n",
    "#     # Read the input Excel file\n",
    "#     input_df = pd.read_excel('input.xlsx')\n",
    "\n",
    "#     # Create a directory to save the articles\n",
    "#     if not os.path.exists('articles'):\n",
    "#         os.makedirs('articles')\n",
    "\n",
    "#     # Prepare the output DataFrame\n",
    "#     output_columns = [\n",
    "#         'URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
    "#         'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE',\n",
    "#         'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'\n",
    "#     ]\n",
    "#     output_df = pd.DataFrame(columns=output_columns)\n",
    "\n",
    "#     # Extract, analyze and save articles\n",
    "#     for index, row in input_df.iterrows():\n",
    "#         url_id = row['URL_ID']\n",
    "#         url = row['URL']\n",
    "#         title, article_text = extract_article_text(url)\n",
    "        \n",
    "#         # Save the extracted text to a file\n",
    "#         with open(f'articles/{url_id}.txt', 'w', encoding='utf-8') as file:\n",
    "#             file.write(title + '\\n\\n' + article_text)\n",
    "        \n",
    "#         # Perform text analysis\n",
    "#         analysis_results = analyze_article(article_text)\n",
    "#         output_row = [url_id, url] + analysis_results\n",
    "#         output_df.loc[len(output_df)] = output_row\n",
    "\n",
    "#     # Save the output to an Excel file\n",
    "#     output_df.to_excel('Output Data Structure.xlsx', index=False)\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def analyze_article(text):\n",
    "    positive_score, negative_score, polarity_score, subjectivity_score = compute_sentiment_scores(text)\n",
    "    avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count, word_count, syllable_count_per_word = compute_readability_metrics(text)\n",
    "    personal_pronouns_count = compute_personal_pronouns(text)\n",
    "    avg_word_length = compute_avg_word_length(text)\n",
    "    \n",
    "    return [\n",
    "        positive_score, negative_score, polarity_score, subjectivity_score,\n",
    "        avg_sentence_length, percentage_complex_words, fog_index,\n",
    "        avg_words_per_sentence, complex_word_count, word_count,\n",
    "        syllable_count_per_word, personal_pronouns_count, avg_word_length\n",
    "    ]\n",
    "\n",
    "def main():\n",
    "    # Prepare the output DataFrame\n",
    "    output_columns = [\n",
    "        'URL_ID', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE',\n",
    "        'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE',\n",
    "        'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'\n",
    "    ]\n",
    "    output_df = pd.DataFrame(columns=output_columns)\n",
    "\n",
    "    # Process each article in the articles directory\n",
    "    articles_dir = 'articles'\n",
    "    for article_file in os.listdir(articles_dir):\n",
    "        if article_file.endswith('.txt'):\n",
    "            url_id = article_file.split('.')[0]\n",
    "            with open(os.path.join(articles_dir, article_file), 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                title, article_text = content.split('\\n\\n', 1)  # Split the title and article text\n",
    "                \n",
    "                # Perform text analysis\n",
    "                analysis_results = analyze_article(article_text)\n",
    "                output_row = [url_id] + analysis_results\n",
    "                output_df.loc[len(output_df)] = output_row\n",
    "\n",
    "    # Save the output to an Excel file\n",
    "    output_df.to_excel('OutputDataStructure.xlsx', index=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
